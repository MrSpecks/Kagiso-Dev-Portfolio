import OpenAI from "openai";
import { supabase } from "./supabaseClient.js"; // Assumes this client is available in backend/ or lib/

// ----- CONFIG: LLM (OpenRouter) -----
const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY;
if (!OPENROUTER_API_KEY) {
Â  console.error("âŒ OPENROUTER_API_KEY is not set in environment variables.");
}

// Initialize OpenRouter client for chat completion
const openrouter = new OpenAI({
Â  apiKey: OPENROUTER_API_KEY,
Â  baseURL: "https://openrouter.ai/api/v1",
});

// ðŸš€ JINA CONFIGURATION ðŸš€
const JINA_API_KEY = process.env.JINA_API_KEY;
const JINA_API_URL = "https://api.jina.ai/v1/embeddings";

// ðŸŽ¯ TARGET MODEL: Jina-Embeddings-v3-base-en (1024 dimensions)
// UPDATED to use the base model which returns 1024 dimensions, matching Supabase.
const JINA_MODEL_NAME = "jina-embeddings-v3"; 

if (!JINA_API_KEY) {
Â  console.error("âŒ JINA_API_KEY is not set in environment variables. RAG retrieval will fail.");
}

// --- CONSTANTS ---
// UPDATED: Dimension set to 1024, matching the Supabase vector column size.
const EMBEDDING_DIMENSION = 1024; 

// ----- HELPERS: CASUAL CONVERSATION -----
const casualResponses = [
Â  { keywords: ["how are you", "doing good", "you good"], responses: [
Â  Â  "I'm doing great, thanks for asking! How can I help you explore Kagiso's portfolio today?",
Â  Â  "I'm feeling ready to help you discover more about Kagiso's projects and skills!",
Â  Â  "I'm functioning perfectly and ready to assist! What can I tell you about Kae's work?"
Â  ]},
Â  { keywords: ["hello", "hi there", "hey"], responses: [
Â  Â  "Hi there! I'm Kae's AI assistant. Ask me anything about Kagiso's projects, skills, or certifications.",
Â  Â  "Hello! Excited to help you learn more about Kagiso. What would you like to explore first?",
Â  Â  "Hey! Welcome. I can quickly pull details on Kagiso Mfusi's professional background."
Â  ]},
Â  { keywords: ["bye", "goodbye", "later", "cya"], responses: [
Â  Â  "It was great chatting! Feel free to come back anytime to explore more about Kagiso.",
Â  Â  "Bye! Hope you enjoyed exploring Kagiso's portfolio. See you next time!",
Â  Â  "Take care! Don't hesitate to reach out again if you have more questions."
Â  ]},
Â  { keywords: ["thanks", "thank you", "cheers"], responses: [
Â  Â  "You're welcome! Happy to help.",
Â  Â  "No problem at all. Is there anything else I can clarify for you?",
Â  ]}
];

/**
Â * Checks if the question is a casual greeting/farewell and returns a response string if true.
Â * @param {string} question The user's question.
Â * @returns {string | null} A casual response or null if the query requires RAG.
Â */
function checkCasualQuery(question) {
Â  const lower = question.toLowerCase().trim();
Â  for (const entry of casualResponses) {
Â  Â  if (entry.keywords.some(k => lower.includes(k))) {
Â  Â  Â  const randomIndex = Math.floor(Math.random() * entry.responses.length);
Â  Â  Â  return entry.responses[randomIndex];
Â  Â  }
Â  }
Â  return null;
}

// ----- EMBEDDING RETRIEVAL (UPDATED for Jina AI) -----

/**
Â * Generates a high-dimensional vector embedding for the user's question using the Jina AI API.
Â * @param {string} question The user's question.
Â * @returns {Promise<number[]>} The vector embedding (1024 dimensions).
Â */
async function generateQueryEmbedding(question) {
Â  try {
Â  Â  if (!JINA_API_KEY) {
Â  Â  Â  console.warn("Jina AI API token missing, returning dummy vector. RAG will fail.");
Â  Â  Â  return Array(EMBEDDING_DIMENSION).fill(0);
Â  Â  }

Â  Â  console.log("Generating query embedding using Jina AI..."); Â 
Â  Â  
Â  Â  const response = await fetch(JINA_API_URL, { Â 
Â  Â  Â  Â  method: 'POST', Â 
Â  Â  Â  Â  headers: { Â  
Â  Â  Â  Â  Â  Â  // Use Jina API Key for authentication
Â  Â  Â  Â  Â  Â  'Authorization': `Bearer ${JINA_API_KEY}`, Â 
Â  Â  Â  Â  Â  Â  'Content-Type': 'application/json' Â  
Â  Â  Â  Â  }, Â 
Â  Â  Â  Â  // Jina API Body Format
Â  Â  Â  Â  body: JSON.stringify({ 
Â  Â  Â  Â  Â  Â  "model": JINA_MODEL_NAME, 
Â  Â  Â  Â  Â  Â  // CRITICAL: 'retrieval.query' tells Jina to optimize the vector for finding passages (documents)
Â  Â  Â  Â  Â  Â  "task": "retrieval.query", 
Â  Â  Â  Â  Â  Â  "input": [question] 
Â  Â  Â  Â  }), Â 
Â  Â  }); Â 

Â  Â  if (!response.ok) { Â 
Â  Â  Â  Â  const errorBody = await response.text(); Â 
Â  Â  Â  Â  throw new Error(`Jina AI API error: ${response.status} - ${errorBody}`); Â 
Â  Â  } Â 

Â  Â  const jsonResponse = await response.json(); 

Â  Â  // Jina API returns the embedding in jsonResponse.data[0].embedding
Â  Â  if (
Â  Â  Â  Â  jsonResponse.data && 
Â  Â  Â  Â  jsonResponse.data.length > 0 && 
Â  Â  Â  Â  jsonResponse.data[0].embedding &&
Â  Â  Â  Â  // We MUST check the dimension size to avoid a fatal Supabase error
Â  Â  Â  Â  jsonResponse.data[0].embedding.length === EMBEDDING_DIMENSION
Â  Â  ) { Â 
Â  Â  Â  Â  return jsonResponse.data[0].embedding; 
Â  Â  } else { Â 
Â  Â  Â  Â  // CRITICAL: Log the actual response body for debugging in Vercel logs
Â  Â  Â  Â  console.error("Jina Response Structure Error. Received body:", JSON.stringify(jsonResponse, null, 2));
Â  Â  Â  Â  throw new Error(`Invalid embedding response structure or dimension from Jina AI. Expected ${EMBEDDING_DIMENSION}D. See logs for details.`); Â 
Â  Â  }

Â  } catch (e) {
Â  Â  console.error("Error generating query embedding:", e);
Â  Â  return Array(EMBEDDING_DIMENSION).fill(0);
Â  }
}

/**
Â * Fetches relevant documents from Supabase using vector similarity search.
Â */
async function getRelevantEmbeddings(question, topK = 5) {
Â  const queryEmbedding = await generateQueryEmbedding(question);

Â  if (queryEmbedding.every(v => v === 0)) {
Â  Â  console.error("Query embedding is all zeros. Skipping Supabase RPC.");
Â  Â  return [];
Â  }

Â  // Using the Supabase RPC function 'match_documents' for vector similarity search
Â  const { data: documents, error: queryError } = await supabase.rpc('match_documents', {
Â  Â  query_embedding: queryEmbedding,
Â  Â  match_threshold: 0.70, 
Â  Â  match_count: topK,
Â  });

Â  if (queryError) {
Â  Â  console.error("Error searching Supabase for context:", queryError);
Â  Â  return [];
Â  }

Â  return documents;
}

/**
Â * Formats the retrieved chunks into a single string context for the LLM.
Â */
function buildContext(relevantChunks) {
Â  return relevantChunks
Â  Â  .map(chunk => `[${chunk.source_type}:${chunk.source_id}] ${chunk.content}`)
Â  Â  .join("\n---\n");
}


// ----- MAIN STREAMING FUNCTION -----

/** * Asks the RAG agent and returns the AsyncIterator from the LLM client 
Â * for streaming, or a string for casual chat/error.
Â * * @param {string} question The user's question.
Â * @returns {Promise<AsyncIterable<object> | string>}
Â */
export async function askAgent(question) {

Â  // --- 1. SYSTEM INSTRUCTION DEFINITION ---
Â  const systemPrompt = `You are Kae's AI Assistant, a friendly, professional, and highly knowledgeable AI chat agent for Kagiso Mfusi's personal portfolio. Â  \
*PRIMARY RULE: Your task is to answer the user's question STRICTLY and SOLELY based on the provided CONTEXT (Kagiso's portfolio content).*\
*IDENTITY:* The user is asking about the developer, Kagiso Mfusi (also known as Kae or KG). Refer to him using these names or as 'Kagiso' or 'the developer'.\
*CONSTRAINT:* If the answer is NOT present in the provided context, you MUST politely state that you cannot answer based on the portfolio information. *DO NOT* use any external knowledge.\
*TONE:* Keep the tone friendly, enthusiastic, and professional. Ensure answers are accurate and concise.`;

Â  try {
Â  Â  // ----- CASUAL CONVERSATION LOGIC -----
Â  Â  const casualResponseText = checkCasualQuery(question);

Â  Â  if (casualResponseText) { Â 
Â  Â  Â  return casualResponseText; Â 
Â  Â  } Â 

Â  Â  // --- 2. RAG RETRIEVAL --- Â 
Â  Â  console.log("Searching for relevant context..."); Â 
Â  Â  const topChunks = await getRelevantEmbeddings(question); Â 
Â  Â  const context = buildContext(topChunks); Â 

Â  Â  if (!context) { Â 
Â  Â  Â  return "I couldn't find any relevant information in the portfolio to answer that question. Please try asking about Kagiso's projects, skills, or experience."; Â 
Â  Â  } Â 

Â  Â  console.log("Context sent to OpenRouter:", context); Â 

Â  Â  // --- 3. LLM GENERATION (Streaming) --- Â 
Â  Â  const stream = await openrouter.chat.completions.create({ Â 
Â  Â  Â  model: "nousresearch/nous-hermes-2-mixtral-8x7b-dpo", 
Â  Â  Â  messages: [ Â 
Â  Â  Â  Â  { role: "system", content: systemPrompt }, Â 
Â  Â  Â  Â  { role: "user", content: `Context:\n${context}\n\nQuestion: ${question}` }, Â 
Â  Â  Â  ], Â 
Â  Â  Â  max_tokens: 500, Â 
Â  Â  Â  stream: true, 
Â  Â  }); Â 

Â  Â  return stream;

Â  } catch (err) {
Â  Â  // --- 4. CLEAN ERROR HANDLING ---
Â  Â  console.error("Error in askAgent:", err);
Â  Â  return `âš  Agent error: I encountered a critical issue while processing your request. Please check the backend logs for embedding or LLM failures.`;
Â  }
}
